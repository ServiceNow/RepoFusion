{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c628ca-dc1c-4083-8ffe-2b7596b5864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e87442-7143-4faa-976e-c560d8cbbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06de5d-826a-4afe-b003-6f52e0c8c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Salesforce/codet5-base\"\n",
    "min_encoder_seq_length = 20\n",
    "min_decoder_seq_length= 20\n",
    "encoder_seq_length=512\n",
    "decoder_seq_length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e852b09-1a5f-4ea5-86be-1bd8a88ab83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_encoder_seq_length+min_decoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b35b5-0d43-4263-81a7-c131e488a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset('/data/hf_repos/the-stack-v1.1/data/java', num_proc=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c0e4f-b33e-4109-9751-f787586f6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds100K = ds['train'].select(range(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3e6e0-e138-406f-8ed2-3c5b74fab055",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a88306-80b4-4412-965a-a95a5f5d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = ds100K.map(lambda xs: tokenizer(xs['content']),batched=True, num_proc=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22eb48-765b-4819-b258-0e66eaa358d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_ds = tokenized_ds.filter(lambda x: len(x['input_ids']) >= min_encoder_seq_length+min_decoder_seq_length, num_proc=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689227c4-5cc7-44e9-ab2a-d84e232b936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b1d5d-15de-4bd0-b348-abc9b62e4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorNTP():\n",
    "    def __init__(\n",
    "        self, tokenizer,\n",
    "        min_encoder_seq_length, min_decoder_seq_length,\n",
    "        encoder_seq_length, decoder_seq_length\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_encoder_seq_length = min_encoder_seq_length\n",
    "        self.min_decoder_seq_length = min_decoder_seq_length\n",
    "        self.encoder_seq_length = encoder_seq_length\n",
    "        self.decoder_seq_length = decoder_seq_length\n",
    "        \n",
    "    def __call__(self, examples, return_tensors=\"pt\"):\n",
    "        \"\"\"\n",
    "        This collate function is to be used with the CodeT5 model, which is a\n",
    "        T5 model with a different tokenizer than the original T5 model.\n",
    "        The tokenization is already done at this point and the examples are a list of\n",
    "        tokenized inputs and labels. We cannot directly train on this data, though,\n",
    "        because CodeT5 expects the inputs to be pre- and appended with special tokens,\n",
    "        and that hasn't been done yet.\n",
    "        The collate function first truncates the inputs and labels to the desired\n",
    "        sequence lengths and then adds the special tokens. The decoder input is\n",
    "        shifted to the right by one position as well.\n",
    "        In particular, the following steps are performed after truncation:\n",
    "        * The encoder input is prepended with the special token 1 (`<s>`) and\n",
    "            appended with the special token 2 (`</s>`). Here's an example of how\n",
    "            that is supposed to look like for CodeT5:\n",
    "                >>> tokenizer = AutoTokenizer.from_pretrained(\"salesforce/codet5-base\", use_fast=True, model_max_lenth=512)\n",
    "                >>> tokenizer(text=\"foo\", add_special_tokens=True).input_ids\n",
    "                [1, 11351, 2]\n",
    "                >>> tokenizer.decode([1, 11351, 2])\n",
    "                '<s>foo</s>'\n",
    "        * The labels are prepended with the special token 1 (`<s>`) and\n",
    "            appended with the special token 2 (`</s>`). Again, here's an example\n",
    "            of how that is supposed to look like for CodeT5:\n",
    "                >>> tokenizer(text_target=\"foo\", add_special_tokens=True).input_ids\n",
    "                [1, 11351, 2]\n",
    "        * For use as decoder inputs, the labels are then shifted to the right by one\n",
    "            position and prepended with the special token 0 (`<pad>`). The special\n",
    "            token 2 (`</s>`) is removed from the end of the labels. For the above\n",
    "            example, this would look like this:\n",
    "                >>> [0] + tokenizer(text_target=\"foo\", add_special_tokens=True).input_ids[:-1]\n",
    "                [0, 1, 11351]\n",
    "        The collate function also adds padding to the inputs and labels. The padding\n",
    "        token is 0 (`<pad>`) for inputs and -100 for labels. The padding is added to\n",
    "        the end of the inputs and labels. The attention mask is padded as well.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(tokenizer, transformers.RobertaTokenizerFast):\n",
    "            raise ValueError(\n",
    "                \"This collate function only works for CodeT5's RobertaTokenizerFast.\"\n",
    "            )\n",
    "\n",
    "        encoder_seq_length = self.encoder_seq_length\n",
    "        decoder_seq_length = self.decoder_seq_length\n",
    "        \n",
    "        # randomly select a midpoint for each example\n",
    "        # the seed is device specific and set by accelerate, so this is deterministic.\n",
    "        # the midpoints will be different each batch and epoch\n",
    "        pivotpoints = [\n",
    "            random.randint(\n",
    "                self.min_encoder_seq_length,\n",
    "                len(example['input_ids'])-self.min_decoder_seq_length\n",
    "            )\n",
    "            for example in examples\n",
    "        ]\n",
    "\n",
    "        # the bos token is used to initialize the decoder_input_ids\n",
    "        bos_token_id = tokenizer.bos_token_id\n",
    "        if bos_token_id is None:\n",
    "            raise ValueError(\"The CodeT5 tokenizer should have a bos token.\")\n",
    "\n",
    "        # the eos token is used to terminate the input_ids, decoder_input_ids, and labels\n",
    "        eos_token_id = tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            raise ValueError(\"The CodeT5 tokenizer should have an eos token.\")\n",
    "\n",
    "        # the pad token is used to initialize the decoder_input_ids\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\"The CodeT5 tokenizer should have a pad token.\")\n",
    "\n",
    "        # a column-wise representation of the examples is needed for the tokenizer.pad function\n",
    "        example_dict: dict[str, list[list[int]]] = {}\n",
    "        # truncate the input_ids to the encoder_seq_length from the beginning,\n",
    "        # add the </s> (EOS) special token at the end, and\n",
    "        # add the <s> (BOS) special token at the beginning if there is enough space\n",
    "        def _add_bos_if_enough_space(\n",
    "            label_seq: list[int], added_val: int = bos_token_id\n",
    "        ) -> list[int]:\n",
    "            if len(label_seq) < encoder_seq_length - 1 and label_seq[0] != bos_token_id:\n",
    "                return [added_val] + label_seq\n",
    "            else:\n",
    "                return label_seq\n",
    "\n",
    "        example_dict[\"input_ids\"] = [\n",
    "            _add_bos_if_enough_space(\n",
    "                example[\"input_ids\"][: pivotpoints[i]][-(encoder_seq_length - 1) :]\n",
    "            )\n",
    "            + [eos_token_id]\n",
    "            for i, example in enumerate(examples)\n",
    "        ]\n",
    "        # truncate the attention_mask to the encoder_seq_length from the beginning\n",
    "        example_dict[\"attention_mask\"] = [\n",
    "            _add_bos_if_enough_space(\n",
    "                example[\"attention_mask\"][: pivotpoints[i]][-(encoder_seq_length - 1) :],\n",
    "                added_val=1,\n",
    "            )\n",
    "            + [1]\n",
    "            for i, example in enumerate(examples)\n",
    "        ]\n",
    "        # add padding to the input_ids and attention_mask\n",
    "        encoding: BatchEncoding = tokenizer.pad(example_dict, return_tensors=\"pt\")\n",
    "\n",
    "        # truncate the labels to the decoder_seq_length from the end,\n",
    "        # add the <s> (BOS) special token at the beginning, and\n",
    "        # add the </s> (EOS) special token at the end if there is enough space\n",
    "        def _add_eos_if_enough_space(label_seq: list[int]) -> list[int]:\n",
    "            if len(label_seq) < decoder_seq_length and label_seq[-1] != eos_token_id:\n",
    "                return label_seq + [eos_token_id]\n",
    "            else:\n",
    "                return label_seq\n",
    "\n",
    "        labels = [\n",
    "            [bos_token_id]\n",
    "            + _add_eos_if_enough_space(\n",
    "                label_seq=example[\"input_ids\"][pivotpoints[i] :][: decoder_seq_length - 1]\n",
    "            )\n",
    "            for i, example in enumerate(examples)\n",
    "        ]\n",
    "        # add padding to the labels using -100 as the ignore_index\n",
    "        max_label_length = max(len(label_seq) for label_seq in labels)\n",
    "        encoding[\"labels\"] = torch.tensor(\n",
    "            [\n",
    "                label_seq + [-100] * (max_label_length - len(label_seq))\n",
    "                for label_seq in labels\n",
    "            ]\n",
    "        )\n",
    "        # shift the labels to the right by one position and\n",
    "        # initialize the decoder_input_ids with the pad token\n",
    "        # (we cannot leave this to `T5ForConditionalGeneration._shift_right` because the labels contain -100)\n",
    "        decoder_input_ids = [\n",
    "            [pad_token_id]\n",
    "            + (\n",
    "                [bos_token_id]\n",
    "                + _add_eos_if_enough_space(\n",
    "                    example[\"input_ids\"][pivotpoints[i] :][: decoder_seq_length - 1]\n",
    "                )\n",
    "            )[:-1]\n",
    "            for i, example in enumerate(examples)\n",
    "        ]\n",
    "        max_decoder_input_length = max(\n",
    "            len(decoder_input_id_seq) for decoder_input_id_seq in decoder_input_ids\n",
    "        )\n",
    "        assert max_decoder_input_length == max_label_length\n",
    "        encoding[\"decoder_input_ids\"] = torch.tensor(\n",
    "            [\n",
    "                decoder_input_id_seq\n",
    "                + [pad_token_id] * (max_decoder_input_length - len(decoder_input_id_seq))\n",
    "                for decoder_input_id_seq in decoder_input_ids\n",
    "            ]\n",
    "        )\n",
    "        return encoding     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f44b5-eed4-4916-809d-4b2116468c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorNTP(\n",
    "    tokenizer,\n",
    "    min_encoder_seq_length = min_encoder_seq_length, min_decoder_seq_length=min_decoder_seq_length,\n",
    "    encoder_seq_length=encoder_seq_length, decoder_seq_length=decoder_seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f4507-6219-4caa-897c-62d14bb2f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_name = \"codet5-base-ntp-java\"\n",
    "model_dir = f\"/repo_data/finetuning_checkpoints/tests/{model_name}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8b45d-c73a-40c4-9512-6ac6659b73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    eval_dataset=None,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9973f8-c905-42c7-ab89-06c049f4708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8cf3a-e660-421d-8082-8b9141096034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
